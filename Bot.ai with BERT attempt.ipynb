{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c67110a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\tiffany mun\\appdata\\roaming\\python\\python311\\site-packages (4.42.4)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\tiffany mun\\appdata\\roaming\\python\\python311\\site-packages (0.23.4)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\tiffany mun\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\tiffany mun\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\tiffany mun\\appdata\\roaming\\python\\python311\\site-packages (from huggingface_hub) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tiffany mun\\appdata\\roaming\\python\\python311\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfbbe45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchinfo in c:\\users\\tiffany mun\\appdata\\roaming\\python\\python311\\site-packages (1.8.0)\n"
     ]
    }
   ],
   "source": [
    "# To get model summary\n",
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12355133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torchinfo import summary\n",
    "import re\n",
    "\n",
    "# Load intents JSON file\n",
    "with open('intents.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract texts and labels\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        texts.append(pattern)\n",
    "        labels.append(intent['tag'])\n",
    "\n",
    "# Encode the labels\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0504a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Set maximum sequence length\n",
    "max_seq_len = 8\n",
    "\n",
    "# Tokenize and encode sequences\n",
    "tokens_train = tokenizer(\n",
    "    texts,\n",
    "    max_length=max_seq_len,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Convert to tensors\n",
    "train_seq = tokens_train['input_ids']\n",
    "train_mask = tokens_train['attention_mask']\n",
    "train_labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a6aec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create TensorDataset\n",
    "train_data = TensorDataset(train_seq, train_mask, train_labels)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78086d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "BERT_Arch                                               --\n",
       "├─DistilBertModel: 1-1                                  --\n",
       "│    └─Embeddings: 2-1                                  --\n",
       "│    │    └─Embedding: 3-1                              (23,440,896)\n",
       "│    │    └─Embedding: 3-2                              (393,216)\n",
       "│    │    └─LayerNorm: 3-3                              (1,536)\n",
       "│    │    └─Dropout: 3-4                                --\n",
       "│    └─Transformer: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-5                             (42,527,232)\n",
       "├─Dropout: 1-2                                          --\n",
       "├─ReLU: 1-3                                             --\n",
       "├─Linear: 1-4                                           393,728\n",
       "├─Linear: 1-5                                           131,328\n",
       "├─Linear: 1-6                                           20,560\n",
       "├─LogSoftmax: 1-7                                       --\n",
       "================================================================================\n",
       "Total params: 66,908,496\n",
       "Trainable params: 545,616\n",
       "Non-trainable params: 66,362,880\n",
       "================================================================================"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the device as CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, len(le.classes_))\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, sent_id, mask):\n",
    "        cls_hs = self.bert(sent_id, attention_mask=mask).last_hidden_state[:, 0]\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Freeze BERT parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Initialize the model\n",
    "bert_classifier = BERT_Arch(model)\n",
    "bert_classifier = bert_classifier.to(device)\n",
    "summary(bert_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "787a7cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TIFFANY MUN\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 200\n",
      "\n",
      "Training Loss: 4.446\n",
      "\n",
      " Epoch 2 / 200\n",
      "\n",
      "Training Loss: 4.388\n",
      "\n",
      " Epoch 3 / 200\n",
      "\n",
      "Training Loss: 4.377\n",
      "\n",
      " Epoch 4 / 200\n",
      "\n",
      "Training Loss: 4.357\n",
      "\n",
      " Epoch 5 / 200\n",
      "\n",
      "Training Loss: 4.322\n",
      "\n",
      " Epoch 6 / 200\n",
      "\n",
      "Training Loss: 4.276\n",
      "\n",
      " Epoch 7 / 200\n",
      "\n",
      "Training Loss: 4.169\n",
      "\n",
      " Epoch 8 / 200\n",
      "\n",
      "Training Loss: 4.023\n",
      "\n",
      " Epoch 9 / 200\n",
      "\n",
      "Training Loss: 3.885\n",
      "\n",
      " Epoch 10 / 200\n",
      "\n",
      "Training Loss: 3.736\n",
      "\n",
      " Epoch 11 / 200\n",
      "\n",
      "Training Loss: 3.599\n",
      "\n",
      " Epoch 12 / 200\n",
      "\n",
      "Training Loss: 3.491\n",
      "\n",
      " Epoch 13 / 200\n",
      "\n",
      "Training Loss: 3.313\n",
      "\n",
      " Epoch 14 / 200\n",
      "\n",
      "Training Loss: 3.213\n",
      "\n",
      " Epoch 15 / 200\n",
      "\n",
      "Training Loss: 3.054\n",
      "\n",
      " Epoch 16 / 200\n",
      "\n",
      "Training Loss: 2.936\n",
      "\n",
      " Epoch 17 / 200\n",
      "\n",
      "Training Loss: 2.840\n",
      "\n",
      " Epoch 18 / 200\n",
      "\n",
      "Training Loss: 2.732\n",
      "\n",
      " Epoch 19 / 200\n",
      "\n",
      "Training Loss: 2.625\n",
      "\n",
      " Epoch 20 / 200\n",
      "\n",
      "Training Loss: 2.526\n",
      "\n",
      " Epoch 21 / 200\n",
      "\n",
      "Training Loss: 2.343\n",
      "\n",
      " Epoch 22 / 200\n",
      "\n",
      "Training Loss: 2.276\n",
      "\n",
      " Epoch 23 / 200\n",
      "\n",
      "Training Loss: 2.179\n",
      "\n",
      " Epoch 24 / 200\n",
      "\n",
      "Training Loss: 1.949\n",
      "\n",
      " Epoch 25 / 200\n",
      "\n",
      "Training Loss: 1.966\n",
      "\n",
      " Epoch 26 / 200\n",
      "\n",
      "Training Loss: 2.036\n",
      "\n",
      " Epoch 27 / 200\n",
      "\n",
      "Training Loss: 1.964\n",
      "\n",
      " Epoch 28 / 200\n",
      "\n",
      "Training Loss: 1.768\n",
      "\n",
      " Epoch 29 / 200\n",
      "\n",
      "Training Loss: 1.754\n",
      "\n",
      " Epoch 30 / 200\n",
      "\n",
      "Training Loss: 1.718\n",
      "\n",
      " Epoch 31 / 200\n",
      "\n",
      "Training Loss: 1.684\n",
      "\n",
      " Epoch 32 / 200\n",
      "\n",
      "Training Loss: 1.553\n",
      "\n",
      " Epoch 33 / 200\n",
      "\n",
      "Training Loss: 1.525\n",
      "\n",
      " Epoch 34 / 200\n",
      "\n",
      "Training Loss: 1.491\n",
      "\n",
      " Epoch 35 / 200\n",
      "\n",
      "Training Loss: 1.428\n",
      "\n",
      " Epoch 36 / 200\n",
      "\n",
      "Training Loss: 1.397\n",
      "\n",
      " Epoch 37 / 200\n",
      "\n",
      "Training Loss: 1.400\n",
      "\n",
      " Epoch 38 / 200\n",
      "\n",
      "Training Loss: 1.350\n",
      "\n",
      " Epoch 39 / 200\n",
      "\n",
      "Training Loss: 1.264\n",
      "\n",
      " Epoch 40 / 200\n",
      "\n",
      "Training Loss: 1.286\n",
      "\n",
      " Epoch 41 / 200\n",
      "\n",
      "Training Loss: 1.232\n",
      "\n",
      " Epoch 42 / 200\n",
      "\n",
      "Training Loss: 1.293\n",
      "\n",
      " Epoch 43 / 200\n",
      "\n",
      "Training Loss: 1.155\n",
      "\n",
      " Epoch 44 / 200\n",
      "\n",
      "Training Loss: 1.179\n",
      "\n",
      " Epoch 45 / 200\n",
      "\n",
      "Training Loss: 1.029\n",
      "\n",
      " Epoch 46 / 200\n",
      "\n",
      "Training Loss: 1.142\n",
      "\n",
      " Epoch 47 / 200\n",
      "\n",
      "Training Loss: 1.078\n",
      "\n",
      " Epoch 48 / 200\n",
      "\n",
      "Training Loss: 1.086\n",
      "\n",
      " Epoch 49 / 200\n",
      "\n",
      "Training Loss: 1.008\n",
      "\n",
      " Epoch 50 / 200\n",
      "\n",
      "Training Loss: 1.086\n",
      "\n",
      " Epoch 51 / 200\n",
      "\n",
      "Training Loss: 0.917\n",
      "\n",
      " Epoch 52 / 200\n",
      "\n",
      "Training Loss: 0.845\n",
      "\n",
      " Epoch 53 / 200\n",
      "\n",
      "Training Loss: 0.828\n",
      "\n",
      " Epoch 54 / 200\n",
      "\n",
      "Training Loss: 0.943\n",
      "\n",
      " Epoch 55 / 200\n",
      "\n",
      "Training Loss: 0.840\n",
      "\n",
      " Epoch 56 / 200\n",
      "\n",
      "Training Loss: 0.862\n",
      "\n",
      " Epoch 57 / 200\n",
      "\n",
      "Training Loss: 0.841\n",
      "\n",
      " Epoch 58 / 200\n",
      "\n",
      "Training Loss: 0.829\n",
      "\n",
      " Epoch 59 / 200\n",
      "\n",
      "Training Loss: 0.882\n",
      "\n",
      " Epoch 60 / 200\n",
      "\n",
      "Training Loss: 0.834\n",
      "\n",
      " Epoch 61 / 200\n",
      "\n",
      "Training Loss: 0.751\n",
      "\n",
      " Epoch 62 / 200\n",
      "\n",
      "Training Loss: 0.898\n",
      "\n",
      " Epoch 63 / 200\n",
      "\n",
      "Training Loss: 0.747\n",
      "\n",
      " Epoch 64 / 200\n",
      "\n",
      "Training Loss: 0.732\n",
      "\n",
      " Epoch 65 / 200\n",
      "\n",
      "Training Loss: 0.723\n",
      "\n",
      " Epoch 66 / 200\n",
      "\n",
      "Training Loss: 0.619\n",
      "\n",
      " Epoch 67 / 200\n",
      "\n",
      "Training Loss: 0.707\n",
      "\n",
      " Epoch 68 / 200\n",
      "\n",
      "Training Loss: 0.666\n",
      "\n",
      " Epoch 69 / 200\n",
      "\n",
      "Training Loss: 0.662\n",
      "\n",
      " Epoch 70 / 200\n",
      "\n",
      "Training Loss: 0.700\n",
      "\n",
      " Epoch 71 / 200\n",
      "\n",
      "Training Loss: 0.612\n",
      "\n",
      " Epoch 72 / 200\n",
      "\n",
      "Training Loss: 0.690\n",
      "\n",
      " Epoch 73 / 200\n",
      "\n",
      "Training Loss: 0.705\n",
      "\n",
      " Epoch 74 / 200\n",
      "\n",
      "Training Loss: 0.637\n",
      "\n",
      " Epoch 75 / 200\n",
      "\n",
      "Training Loss: 0.570\n",
      "\n",
      " Epoch 76 / 200\n",
      "\n",
      "Training Loss: 0.587\n",
      "\n",
      " Epoch 77 / 200\n",
      "\n",
      "Training Loss: 0.677\n",
      "\n",
      " Epoch 78 / 200\n",
      "\n",
      "Training Loss: 0.633\n",
      "\n",
      " Epoch 79 / 200\n",
      "\n",
      "Training Loss: 0.549\n",
      "\n",
      " Epoch 80 / 200\n",
      "\n",
      "Training Loss: 0.524\n",
      "\n",
      " Epoch 81 / 200\n",
      "\n",
      "Training Loss: 0.558\n",
      "\n",
      " Epoch 82 / 200\n",
      "\n",
      "Training Loss: 0.647\n",
      "\n",
      " Epoch 83 / 200\n",
      "\n",
      "Training Loss: 0.521\n",
      "\n",
      " Epoch 84 / 200\n",
      "\n",
      "Training Loss: 0.520\n",
      "\n",
      " Epoch 85 / 200\n",
      "\n",
      "Training Loss: 0.517\n",
      "\n",
      " Epoch 86 / 200\n",
      "\n",
      "Training Loss: 0.514\n",
      "\n",
      " Epoch 87 / 200\n",
      "\n",
      "Training Loss: 0.454\n",
      "\n",
      " Epoch 88 / 200\n",
      "\n",
      "Training Loss: 0.500\n",
      "\n",
      " Epoch 89 / 200\n",
      "\n",
      "Training Loss: 0.508\n",
      "\n",
      " Epoch 90 / 200\n",
      "\n",
      "Training Loss: 0.474\n",
      "\n",
      " Epoch 91 / 200\n",
      "\n",
      "Training Loss: 0.526\n",
      "\n",
      " Epoch 92 / 200\n",
      "\n",
      "Training Loss: 0.474\n",
      "\n",
      " Epoch 93 / 200\n",
      "\n",
      "Training Loss: 0.480\n",
      "\n",
      " Epoch 94 / 200\n",
      "\n",
      "Training Loss: 0.471\n",
      "\n",
      " Epoch 95 / 200\n",
      "\n",
      "Training Loss: 0.469\n",
      "\n",
      " Epoch 96 / 200\n",
      "\n",
      "Training Loss: 0.351\n",
      "\n",
      " Epoch 97 / 200\n",
      "\n",
      "Training Loss: 0.427\n",
      "\n",
      " Epoch 98 / 200\n",
      "\n",
      "Training Loss: 0.462\n",
      "\n",
      " Epoch 99 / 200\n",
      "\n",
      "Training Loss: 0.506\n",
      "\n",
      " Epoch 100 / 200\n",
      "\n",
      "Training Loss: 0.380\n",
      "\n",
      " Epoch 101 / 200\n",
      "\n",
      "Training Loss: 0.409\n",
      "\n",
      " Epoch 102 / 200\n",
      "\n",
      "Training Loss: 0.456\n",
      "\n",
      " Epoch 103 / 200\n",
      "\n",
      "Training Loss: 0.426\n",
      "\n",
      " Epoch 104 / 200\n",
      "\n",
      "Training Loss: 0.441\n",
      "\n",
      " Epoch 105 / 200\n",
      "\n",
      "Training Loss: 0.380\n",
      "\n",
      " Epoch 106 / 200\n",
      "\n",
      "Training Loss: 0.349\n",
      "\n",
      " Epoch 107 / 200\n",
      "\n",
      "Training Loss: 0.360\n",
      "\n",
      " Epoch 108 / 200\n",
      "\n",
      "Training Loss: 0.362\n",
      "\n",
      " Epoch 109 / 200\n",
      "\n",
      "Training Loss: 0.322\n",
      "\n",
      " Epoch 110 / 200\n",
      "\n",
      "Training Loss: 0.364\n",
      "\n",
      " Epoch 111 / 200\n",
      "\n",
      "Training Loss: 0.370\n",
      "\n",
      " Epoch 112 / 200\n",
      "\n",
      "Training Loss: 0.343\n",
      "\n",
      " Epoch 113 / 200\n",
      "\n",
      "Training Loss: 0.409\n",
      "\n",
      " Epoch 114 / 200\n",
      "\n",
      "Training Loss: 0.431\n",
      "\n",
      " Epoch 115 / 200\n",
      "\n",
      "Training Loss: 0.342\n",
      "\n",
      " Epoch 116 / 200\n",
      "\n",
      "Training Loss: 0.338\n",
      "\n",
      " Epoch 117 / 200\n",
      "\n",
      "Training Loss: 0.418\n",
      "\n",
      " Epoch 118 / 200\n",
      "\n",
      "Training Loss: 0.250\n",
      "\n",
      " Epoch 119 / 200\n",
      "\n",
      "Training Loss: 0.362\n",
      "\n",
      " Epoch 120 / 200\n",
      "\n",
      "Training Loss: 0.349\n",
      "\n",
      " Epoch 121 / 200\n",
      "\n",
      "Training Loss: 0.332\n",
      "\n",
      " Epoch 122 / 200\n",
      "\n",
      "Training Loss: 0.339\n",
      "\n",
      " Epoch 123 / 200\n",
      "\n",
      "Training Loss: 0.311\n",
      "\n",
      " Epoch 124 / 200\n",
      "\n",
      "Training Loss: 0.425\n",
      "\n",
      " Epoch 125 / 200\n",
      "\n",
      "Training Loss: 0.505\n",
      "\n",
      " Epoch 126 / 200\n",
      "\n",
      "Training Loss: 0.388\n",
      "\n",
      " Epoch 127 / 200\n",
      "\n",
      "Training Loss: 0.279\n",
      "\n",
      " Epoch 128 / 200\n",
      "\n",
      "Training Loss: 0.307\n",
      "\n",
      " Epoch 129 / 200\n",
      "\n",
      "Training Loss: 0.351\n",
      "\n",
      " Epoch 130 / 200\n",
      "\n",
      "Training Loss: 0.388\n",
      "\n",
      " Epoch 131 / 200\n",
      "\n",
      "Training Loss: 0.358\n",
      "\n",
      " Epoch 132 / 200\n",
      "\n",
      "Training Loss: 0.350\n",
      "\n",
      " Epoch 133 / 200\n",
      "\n",
      "Training Loss: 0.305\n",
      "\n",
      " Epoch 134 / 200\n",
      "\n",
      "Training Loss: 0.254\n",
      "\n",
      " Epoch 135 / 200\n",
      "\n",
      "Training Loss: 0.330\n",
      "\n",
      " Epoch 136 / 200\n",
      "\n",
      "Training Loss: 0.302\n",
      "\n",
      " Epoch 137 / 200\n",
      "\n",
      "Training Loss: 0.278\n",
      "\n",
      " Epoch 138 / 200\n",
      "\n",
      "Training Loss: 0.230\n",
      "\n",
      " Epoch 139 / 200\n",
      "\n",
      "Training Loss: 0.323\n",
      "\n",
      " Epoch 140 / 200\n",
      "\n",
      "Training Loss: 0.274\n",
      "\n",
      " Epoch 141 / 200\n",
      "\n",
      "Training Loss: 0.356\n",
      "\n",
      " Epoch 142 / 200\n",
      "\n",
      "Training Loss: 0.378\n",
      "\n",
      " Epoch 143 / 200\n",
      "\n",
      "Training Loss: 0.221\n",
      "\n",
      " Epoch 144 / 200\n",
      "\n",
      "Training Loss: 0.219\n",
      "\n",
      " Epoch 145 / 200\n",
      "\n",
      "Training Loss: 0.250\n",
      "\n",
      " Epoch 146 / 200\n",
      "\n",
      "Training Loss: 0.278\n",
      "\n",
      " Epoch 147 / 200\n",
      "\n",
      "Training Loss: 0.272\n",
      "\n",
      " Epoch 148 / 200\n",
      "\n",
      "Training Loss: 0.302\n",
      "\n",
      " Epoch 149 / 200\n",
      "\n",
      "Training Loss: 0.330\n",
      "\n",
      " Epoch 150 / 200\n",
      "\n",
      "Training Loss: 0.370\n",
      "\n",
      " Epoch 151 / 200\n",
      "\n",
      "Training Loss: 0.299\n",
      "\n",
      " Epoch 152 / 200\n",
      "\n",
      "Training Loss: 0.268\n",
      "\n",
      " Epoch 153 / 200\n",
      "\n",
      "Training Loss: 0.262\n",
      "\n",
      " Epoch 154 / 200\n",
      "\n",
      "Training Loss: 0.324\n",
      "\n",
      " Epoch 155 / 200\n",
      "\n",
      "Training Loss: 0.208\n",
      "\n",
      " Epoch 156 / 200\n",
      "\n",
      "Training Loss: 0.290\n",
      "\n",
      " Epoch 157 / 200\n",
      "\n",
      "Training Loss: 0.248\n",
      "\n",
      " Epoch 158 / 200\n",
      "\n",
      "Training Loss: 0.208\n",
      "\n",
      " Epoch 159 / 200\n",
      "\n",
      "Training Loss: 0.350\n",
      "\n",
      " Epoch 160 / 200\n",
      "\n",
      "Training Loss: 0.219\n",
      "\n",
      " Epoch 161 / 200\n",
      "\n",
      "Training Loss: 0.296\n",
      "\n",
      " Epoch 162 / 200\n",
      "\n",
      "Training Loss: 0.269\n",
      "\n",
      " Epoch 163 / 200\n",
      "\n",
      "Training Loss: 0.195\n",
      "\n",
      " Epoch 164 / 200\n",
      "\n",
      "Training Loss: 0.207\n",
      "\n",
      " Epoch 165 / 200\n",
      "\n",
      "Training Loss: 0.237\n",
      "\n",
      " Epoch 166 / 200\n",
      "\n",
      "Training Loss: 0.289\n",
      "\n",
      " Epoch 167 / 200\n",
      "\n",
      "Training Loss: 0.311\n",
      "\n",
      " Epoch 168 / 200\n",
      "\n",
      "Training Loss: 0.268\n",
      "\n",
      " Epoch 169 / 200\n",
      "\n",
      "Training Loss: 0.252\n",
      "\n",
      " Epoch 170 / 200\n",
      "\n",
      "Training Loss: 0.231\n",
      "\n",
      " Epoch 171 / 200\n",
      "\n",
      "Training Loss: 0.253\n",
      "\n",
      " Epoch 172 / 200\n",
      "\n",
      "Training Loss: 0.191\n",
      "\n",
      " Epoch 173 / 200\n",
      "\n",
      "Training Loss: 0.365\n",
      "\n",
      " Epoch 174 / 200\n",
      "\n",
      "Training Loss: 0.248\n",
      "\n",
      " Epoch 175 / 200\n",
      "\n",
      "Training Loss: 0.194\n",
      "\n",
      " Epoch 176 / 200\n",
      "\n",
      "Training Loss: 0.310\n",
      "\n",
      " Epoch 177 / 200\n",
      "\n",
      "Training Loss: 0.153\n",
      "\n",
      " Epoch 178 / 200\n",
      "\n",
      "Training Loss: 0.248\n",
      "\n",
      " Epoch 179 / 200\n",
      "\n",
      "Training Loss: 0.223\n",
      "\n",
      " Epoch 180 / 200\n",
      "\n",
      "Training Loss: 0.163\n",
      "\n",
      " Epoch 181 / 200\n",
      "\n",
      "Training Loss: 0.180\n",
      "\n",
      " Epoch 182 / 200\n",
      "\n",
      "Training Loss: 0.290\n",
      "\n",
      " Epoch 183 / 200\n",
      "\n",
      "Training Loss: 0.202\n",
      "\n",
      " Epoch 184 / 200\n",
      "\n",
      "Training Loss: 0.185\n",
      "\n",
      " Epoch 185 / 200\n",
      "\n",
      "Training Loss: 0.284\n",
      "\n",
      " Epoch 186 / 200\n",
      "\n",
      "Training Loss: 0.320\n",
      "\n",
      " Epoch 187 / 200\n",
      "\n",
      "Training Loss: 0.243\n",
      "\n",
      " Epoch 188 / 200\n",
      "\n",
      "Training Loss: 0.138\n",
      "\n",
      " Epoch 189 / 200\n",
      "\n",
      "Training Loss: 0.231\n",
      "\n",
      " Epoch 190 / 200\n",
      "\n",
      "Training Loss: 0.191\n",
      "\n",
      " Epoch 191 / 200\n",
      "\n",
      "Training Loss: 0.239\n",
      "\n",
      " Epoch 192 / 200\n",
      "\n",
      "Training Loss: 0.200\n",
      "\n",
      " Epoch 193 / 200\n",
      "\n",
      "Training Loss: 0.241\n",
      "\n",
      " Epoch 194 / 200\n",
      "\n",
      "Training Loss: 0.177\n",
      "\n",
      " Epoch 195 / 200\n",
      "\n",
      "Training Loss: 0.218\n",
      "\n",
      " Epoch 196 / 200\n",
      "\n",
      "Training Loss: 0.227\n",
      "\n",
      " Epoch 197 / 200\n",
      "\n",
      "Training Loss: 0.181\n",
      "\n",
      " Epoch 198 / 200\n",
      "\n",
      "Training Loss: 0.254\n",
      "\n",
      " Epoch 199 / 200\n",
      "\n",
      "Training Loss: 0.236\n",
      "\n",
      " Epoch 200 / 200\n",
      "\n",
      "Training Loss: 0.154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(bert_classifier.parameters(), lr=1e-3)\n",
    "\n",
    "# Compute class weights\n",
    "class_wts = torch.tensor(compute_class_weight('balanced', classes=np.unique(labels), y=labels), dtype=torch.float).to(device)\n",
    "cross_entropy = nn.NLLLoss(weight=class_wts)\n",
    "\n",
    "# Train the model\n",
    "def train():\n",
    "    bert_classifier.train()\n",
    "    total_loss = 0\n",
    "    total_preds = []\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        preds = bert_classifier(sent_id, mask)\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(bert_classifier.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = preds.detach().numpy()\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "    return avg_loss, total_preds\n",
    "\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    train_loss, _ = train()\n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af761560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent: greeting\n",
      "Response: Hello there. Tell me how are you feeling today?\n"
     ]
    }
   ],
   "source": [
    "def get_prediction(text):\n",
    "    text = re.sub(r'[^a-zA-Z ]+', '', text)\n",
    "    test_text = [text]\n",
    "    tokens_test_data = tokenizer(\n",
    "        test_text,\n",
    "        max_length=max_seq_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    test_seq = tokens_test_data['input_ids']\n",
    "    test_mask = tokens_test_data['attention_mask']\n",
    "\n",
    "    bert_classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = bert_classifier(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return le.inverse_transform(preds)[0]\n",
    "\n",
    "def get_response(message):\n",
    "    intent = get_prediction(message)\n",
    "    for i in data['intents']:\n",
    "        if i['tag'] == intent:\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    return \"Intent: \" + intent + '\\n' + \"Response: \" + result\n",
    "\n",
    "# Example usage\n",
    "print(get_response(\"Hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7174217e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the bot (type 'quit' to stop)!\n",
      "You: hi\n",
      "Bot: Intent: greeting\n",
      "Response: Hello there. Glad to see you're back. What's going on in your world right now?\n",
      "You: not much\n",
      "Bot: Intent: neutral-response\n",
      "Response: Oh I see. Do you want to talk about something?\n",
      "You: okay\n",
      "Bot: Intent: casual\n",
      "Response: Can you elaborate on that?\n",
      "You: im going through a tough time\n",
      "Bot: Intent: not-talking\n",
      "Response: Talking about something really helps. If you're not ready to open up then that's ok. Just know that i'm here for you, whenever you need me.\n",
      "You: alright\n",
      "Bot: Intent: casual\n",
      "Response: Let's discuss further why you're feeling this way.\n",
      "You: i lost my necklace\n",
      "Bot: Intent: death\n",
      "Response: I'm sorry to hear that. If you want to talk about it. I'm here.\n",
      "You: it was expensive\n",
      "Bot: Intent: casual\n",
      "Response: Can you elaborate on that?\n",
      "You: it was $500\n",
      "Bot: Intent: neutral-response\n",
      "Response: Oh I see. Do you want to talk about something?\n",
      "You: ok bye\n",
      "Bot: Intent: goodbye\n",
      "Response: Have a nice day.\n"
     ]
    }
   ],
   "source": [
    "# Conversational loop\n",
    "def chat():\n",
    "    print(\"Start chatting with the bot (type 'quit' to stop)!\")\n",
    "    while True:\n",
    "        message = input(\"You: \")\n",
    "        if message.lower() == 'quit':\n",
    "            print(\"Bot: Goodbye!\")\n",
    "            break\n",
    "        response = get_response(message)\n",
    "        print(f\"Bot: {response}\")\n",
    "\n",
    "# Start the chatbot conversation\n",
    "chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
